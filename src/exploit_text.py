import sys

sys.path.append(r"./..")

import os
import re
from collections import Counter
from statistics import mean, median

import matplotlib.pyplot as plt
import pandas as pd
from resources.stop_words_french import french_stop_words
from wordcloud import WordCloud

# from transformers import AutoTokenizer

# tokenizer = AutoTokenizer.from_pretrained("bert_base_cased")

TXT_PATH = "txt"
stop_words = french_stop_words()

files = [f.name for f in os.scandir(TXT_PATH) if os.path.isfile(f)]
texts = []
for file in files:
    with open(os.path.join(TXT_PATH, file), "r", encoding="utf-8") as current_file:
        texts.append(current_file.read())

df = pd.DataFrame(texts, columns=["text"])

len_text_list = []
nb_phrases_list = []
vocab_list = []
# nb_token_list = []

for text in df.text:
    len_text_list.append(len(text.split()))
    nb_phrases_list.append(len(text.split(".")))
    vocab_list.append(text.split())
    # nb_token_list.append(len(tokenizer.tokenize(text)))

print(
    f"""longueur moyenne : {mean(len_text_list)}
        longueur mediane : {median(len_text_list)}
        nb phrases moyenne: {mean(nb_phrases_list)}
        nb phrases median : {median(nb_phrases_list)}"""
)

vocab = [
    item.lower()
    for sublist in vocab_list
    for item in sublist
    if (item.lower() not in stop_words and re.match("[A-z]", item) and len(item) > 2)
]
counter = Counter(vocab)

wordcloud = WordCloud(background_color="white").generate_from_frequencies(
    dict(counter.most_common(50))
)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

fig, ax = plt.subplots()
ax.hist(len_text_list, bins=20)
plt.show()

print(f"most 10 common words : {counter.most_common(50)}")
